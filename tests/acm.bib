@article{10.1145/3786344,
author = {He, Xu and Meng, Xiaolin and Mo, Lingfei and Zhang, Youdong and Yu, Fangwen and Liu, Jingnan},
title = {A Comprehensive Review of Brain-inspired Navigation},
year = {2026},
issue_date = {June 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3786344},
doi = {10.1145/3786344},
abstract = {Intelligent navigation is essential for unmanned systems. Yet nowadays navigation technologies still fall short of animals’ innate navigation prowess, characterized by continuous, efficient, adaptive, low-power navigating across complex terrains, and despite technological advancements. Neuroscience's half-century exploration has revealed the brain's innate “Global Positioning System (GPS),” instigating research into Brain-Inspired Navigation (BIN). BIN, is a cutting-edge navigation technology, that bridges disciplines but lacks a cohesive guide for its interdisciplinary study. In this article, we offer a comprehensive BIN review, mapping its neural basis, computational foundations, current progress, and implementation conditions, providing a general framework for researchers alongside forward-looking recommendations for future development in the domain. The highlights of this article can be available at .},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {200},
numpages = {51},
keywords = {Brain-inspired navigation, general framework, literature review}
}

@article{10.1145/3786332,
author = {Wang, Haoyang and Guo, Ruishan and Ma, Pengtao and Ruan, Ciyu and Luo, Xinyu and Ding, Wenhua and Zhong, Tianyang and Xu, Jingao and Liu, Yunhao and Chen, Xinlei},
title = {Event Camera Meets Mobile Embodied Perception: Abstraction, Algorithm, Acceleration, Application},
year = {2026},
issue_date = {June 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3786332},
doi = {10.1145/3786332},
abstract = {With the evolution of mobile embodied intelligence, agents such as drones and autonomous robots are transitioning toward high agility. This shift imposes stringent demands on embodied perception, requiring high-accuracy and low-latency feedback loops for reliable interaction. Event-based vision has emerged as a transformative paradigm. Its microsecond-level temporal resolution and high dynamic range render it ideal for embodied perception tasks on high-agility mobile platforms. However, asynchronous nature, substantial noise, lack of persistent semantic information, and large data volume pose challenges for processing on resource-constrained mobile agents. This article surveys the literature from 2014–2025 and presents a comprehensive overview of event-based mobile embodied perception. We organize review around four key pillars: event abstraction methods, perception algorithm advancements, hardware and software acceleration strategies, and mobile applications. We discuss critical tasks including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with sensor fusion and real-time deployment. Furthermore, we outline future research directions, such as improving event cameras with advanced optics and leveraging neuromorphic computing for efficient processing. To support ongoing research, we provide an open-source Online Sheet with recent developments. We hope this survey serves as a reference, facilitating adoption of event-based vision across diverse mobile embodied applications.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {206},
numpages = {41},
keywords = {Embodied AI, mobile embodied perception, event camera, event-based vision}
}

